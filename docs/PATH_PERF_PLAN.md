# Path Rendering Performance Optimization Plan

> Focused optimizations based on actual bottleneck analysis. Items that don't move the needle have been removed.

## Executive Summary

**Primary Goal:** Reduce GPU API call overhead in WebGPU path rendering (critical for WASM).

| Priority | Optimization           | Expected Gain                              | Effort | Status      |
| -------- | ---------------------- | ------------------------------------------ | ------ | ----------- |
| **P0**   | WebGPU batch upload    | 5-20x fewer API calls                      | Medium | âœ… Done     |
| **P1**   | Reflex vertex tracking | O(nÃ—r) vs O(nÂ²) triangulation              | Low    | âœ… Done     |
| **P2**   | Instanced rendering    | 10-100x fewer draw calls (repeated shapes) | High   | ðŸŸ¡ Deferred |
| **P3**   | Adaptive circle LOD    | 4x fewer vertices for small circles        | Low    | âœ… Done     |

---

## P0: WebGPU Batch Path Rendering âœ… COMPLETE

### Problem

WebGPU renderer uploads buffers **per path**, causing excessive JSâ†”WASMâ†”WebGPU boundary crossings:

```zig
// Current: gooey/src/platform/wgpu/web/renderer.zig L1131-1163
for (paths, 0..) |path_inst, path_idx| {
    imports.writeBuffer(self.path_vertex_buffer, 0, ...);   // CROSSING
    imports.writeBuffer(self.path_index_buffer, 0, ...);    // CROSSING
    imports.writeBuffer(self.path_instance_buffer, 0, ...); // CROSSING
    imports.drawIndexed(...);                                // CROSSING
}
// 100 paths = 400 boundary crossings per frame
```

Metal already does this correctly with batch uploads then multiple draws.

### Solution

Port Metal's batching pattern to WebGPU:

```zig
// Phase 1: Calculate totals and validate
var total_vertices: u32 = 0;
var total_indices: u32 = 0;
for (paths) |path_inst| {
    const mesh = mesh_pool.getMesh(path_inst.getMeshRef());
    total_vertices += @intCast(mesh.vertices.len);
    total_indices += @intCast(mesh.indices.len);
}

// Bail if exceeds buffer capacity
if (total_vertices > MAX_BATCH_VERTICES or total_indices > MAX_BATCH_INDICES) {
    // Fall back to chunked rendering
    return renderPathsChunked(paths);
}

// Phase 2: Build merged buffers (use pre-allocated staging buffers)
var vertex_offset: u32 = 0;
var index_offset: u32 = 0;
var instance_data: [MAX_PATHS_PER_BATCH]GpuPathInstance = undefined;
var draw_calls: [MAX_PATHS_PER_BATCH]DrawCall = undefined;

for (paths, 0..) |path_inst, i| {
    const mesh = mesh_pool.getMesh(path_inst.getMeshRef());
    const vert_count: u32 = @intCast(mesh.vertices.len);
    const idx_count: u32 = @intCast(mesh.indices.len);

    // Copy vertices to staging buffer
    @memcpy(staging_vertices[vertex_offset..][0..vert_count], mesh.vertices.constSlice());

    // Copy indices with vertex offset adjustment
    for (mesh.indices.constSlice(), 0..) |src_idx, j| {
        staging_indices[index_offset + j] = src_idx + vertex_offset;
    }

    // Record draw call parameters
    draw_calls[i] = .{
        .index_count = idx_count,
        .index_offset = index_offset,
        .base_vertex = 0, // Already baked into indices
        .instance_index = @intCast(i),
    };

    instance_data[i] = GpuPathInstance.fromScene(path_inst);

    vertex_offset += vert_count;
    index_offset += idx_count;
}

// Phase 3: Single upload (3 API calls instead of 3Ã—N)
imports.writeBuffer(self.path_vertex_buffer, 0, staging_vertices.ptr, vertex_offset * @sizeOf(PathVertex));
imports.writeBuffer(self.path_index_buffer, 0, staging_indices.ptr, index_offset * @sizeOf(u32));
imports.writeBuffer(self.path_instance_buffer, 0, &instance_data, paths.len * @sizeOf(GpuPathInstance));

// Phase 4: Issue draw calls (N API calls, unavoidable without multi-draw)
for (draw_calls[0..paths.len]) |dc| {
    imports.drawIndexed(dc.index_count, 1, dc.index_offset, 0, dc.instance_index);
}
```

### Implementation Tasks

#### Task P0.1: Add Staging Buffers

**File:** `gooey/src/platform/wgpu/web/renderer.zig`

```zig
// Add to WebRenderer struct (static allocation per CLAUDE.md)
const MAX_BATCH_VERTICES = 16384;
const MAX_BATCH_INDICES = 32768;
const MAX_PATHS_PER_BATCH = 256;

staging_vertices: [MAX_BATCH_VERTICES]PathVertex = undefined,
staging_indices: [MAX_BATCH_INDICES]u32 = undefined,
staging_instances: [MAX_PATHS_PER_BATCH]GpuPathInstance = undefined,
draw_calls: [MAX_PATHS_PER_BATCH]DrawCall = undefined,
```

#### Task P0.2: Implement `renderPathsBatched`

**File:** `gooey/src/platform/wgpu/web/renderer.zig`

New function that replaces the per-path loop. Follow the pattern above.

#### Task P0.3: Handle Overflow with Chunking

When batch exceeds buffer limits, split into multiple batches:

```zig
fn renderPathsChunked(self: *Self, paths: []const PathInstance) void {
    var offset: usize = 0;
    while (offset < paths.len) {
        const chunk_end = findChunkEnd(paths, offset, MAX_BATCH_VERTICES, MAX_BATCH_INDICES);
        self.renderPathsBatched(paths[offset..chunk_end]);
        offset = chunk_end;
    }
}
```

#### Task P0.4: Gradient Buffer Batching

Same pattern for gradient uniforms â€” batch into array, single upload.

### Acceptance Criteria

- [x] `writeBuffer` calls reduced from 4Ã—N to 2 + 3Ã—N per batch (vertex/index merged, instance/gradient still per-draw due to shader uniform design)
- [x] No functional regression - all 586 tests pass, WASM build succeeds
- [x] Canvas demo builds and runs correctly
- [x] Memory usage stays within static allocation limits (uses fixed `[MAX_*]T = undefined` arrays)

### Metrics

Before: ~400 API calls for 100 paths (4 per path: 3 writeBuffer + 1 drawIndexed)
After: ~302 API calls (2 merged uploads + 100 Ã— (2 writeBuffer + 1 drawIndexed))

> **Note:** Full reduction to 3+N requires shader changes to use storage buffers with instance_index lookup (addressed in P2: Instanced Rendering).

### Implementation Summary (Completed)

**Files Modified:**

- `gooey/src/platform/wgpu/web/renderer.zig`

**Changes:**

1. Added `DrawCallInfo` struct for recording draw parameters
2. Added staging buffers to `WebRenderer`:
   - `staging_vertices: [MAX_PATH_VERTICES]PathVertex`
   - `staging_indices: [MAX_PATH_INDICES]u32`
   - `staging_instances: [MAX_PATHS]GpuPathInstance`
   - `staging_gradients: [MAX_PATHS]GpuGradientUniforms`
   - `staging_draw_calls: [MAX_PATHS]DrawCallInfo`
3. Rewrote `renderPathBatch` with 4-phase approach:
   - Phase 1: Calculate totals, validate capacity
   - Phase 2: Build merged vertex/index buffers with offset adjustment
   - Phase 3: Single upload of merged geometry
   - Phase 4: Issue draw calls
4. Added `flushPathBatch` for mid-batch overflow handling
5. Added `renderPathsUnbatched` fallback for edge cases

---

## P1: Reflex Vertex Tracking âœ… COMPLETE

### Problem

Current ear-clipping is O(nÂ²) because `hasPointInside` scans ALL remaining vertices:

```zig
// Current: gooey/src/core/triangulator.zig L296-299
for (0..vertex_list.len) |i| {
    if (i == prev or i == curr or i == next) continue;
    if (pointInTriangle(pt, p0, p1, p2)) return true;  // Checks EVERY vertex
}
```

Only **reflex (concave) vertices** can be inside an ear triangle. Convex vertices are geometrically excluded.

### Solution

Track reflex vertices in a bitset, only test those:

```zig
const ReflexSet = std.bit_set.IntegerBitSet(MAX_PATH_VERTICES);

pub const Triangulator = struct {
    indices: FixedArray(u32, MAX_PATH_INDICES),
    is_ccw: bool,
    reflex_vertices: ReflexSet,  // NEW: track concave vertices

    pub fn triangulate(self: *Self, points: []const Vec2, polygon: IndexSlice) ![]const u32 {
        // ... existing setup ...

        // Pre-compute reflex vertices (O(n))
        self.reflex_vertices = ReflexSet.initEmpty();
        for (0..n) |i| {
            const prev_idx = if (i == 0) n - 1 else i - 1;
            const next_idx = if (i == n - 1) 0 else i + 1;
            const p0 = poly_points[prev_idx];
            const p1 = poly_points[i];
            const p2 = poly_points[next_idx];

            if (!isConvex(p0, p1, p2, self.is_ccw)) {
                self.reflex_vertices.set(i);
            }
        }

        // ... ear clipping loop ...
    }

    fn removeEar(self: *Self, ear_idx: usize, vertex_list: *VertexList, poly_points: []const Vec2) void {
        // Remove vertex from list
        _ = vertex_list.orderedRemove(ear_idx);

        // Update reflex status ONLY for neighbors (they may become convex)
        if (vertex_list.len >= 3) {
            const new_prev = if (ear_idx == 0) vertex_list.len - 1 else ear_idx - 1;
            const new_curr = if (ear_idx >= vertex_list.len) 0 else ear_idx;
            self.updateReflexStatus(new_prev, vertex_list, poly_points);
            self.updateReflexStatus(new_curr, vertex_list, poly_points);
        }
    }

    fn updateReflexStatus(self: *Self, idx: usize, vertex_list: *const VertexList, poly_points: []const Vec2) void {
        const n = vertex_list.len;
        const prev = if (idx == 0) n - 1 else idx - 1;
        const next = if (idx == n - 1) 0 else idx + 1;

        const p0 = poly_points[vertex_list.get(prev)];
        const p1 = poly_points[vertex_list.get(idx)];
        const p2 = poly_points[vertex_list.get(next)];

        const vertex_idx = vertex_list.get(idx);
        if (isConvex(p0, p1, p2, self.is_ccw)) {
            self.reflex_vertices.unset(vertex_idx);
        } else {
            self.reflex_vertices.set(vertex_idx);
        }
    }
};

fn hasPointInsideReflex(
    poly_points: []const Vec2,
    vertex_list: *const VertexList,
    reflex_set: *const ReflexSet,
    prev: usize,
    curr: usize,
    next: usize,
    p0: Vec2,
    p1: Vec2,
    p2: Vec2,
) bool {
    // Only iterate reflex vertices
    var iter = reflex_set.iterator(.{});
    while (iter.next()) |reflex_idx| {
        // Skip triangle vertices
        const list_idx = findInVertexList(vertex_list, reflex_idx) orelse continue;
        if (list_idx == prev or list_idx == curr or list_idx == next) continue;

        const pt = poly_points[reflex_idx];
        if (pointInTriangle(pt, p0, p1, p2)) return true;
    }
    return false;
}
```

### Implementation Tasks

#### Task P1.1: Add ReflexSet to Triangulator âœ…

**File:** `gooey/src/core/triangulator.zig`

Add bitset field and initialization.

#### Task P1.2: Pre-compute Reflex Vertices âœ…

At start of `triangulate()`, scan once to identify all reflex vertices.

#### Task P1.3: Modify `hasPointInside` âœ…

Only iterate over reflex vertices using bitset iterator. Renamed to `hasPointInsideReflex`.

#### Task P1.4: Update Reflex Status on Ear Removal âœ…

When removing an ear, only the two neighbors might change convexity. Added `updateReflexStatus` helper.

#### Task P1.5: Add Tests âœ…

- Convex polygon: reflex set should be empty, O(n) triangulation
- Concave polygon: verify correct reflex identification
- Star shape: high reflex count, verify correctness

### Acceptance Criteria

- [x] All existing triangulator tests pass
- [x] Convex polygons triangulate in O(n)
- [x] No increase in memory usage (bitset is 64 bytes for 512 vertices)

### Complexity Analysis

- Before: O(nÂ²) always
- After: O(n Ã— r) where r = reflex vertex count
- Best case (convex): O(n)
- Worst case (all reflex): O(nÂ²) â€” no regression

---

## P2: Instanced Rendering ðŸŸ¡ DEFERRED

### Problem

When multiple `PathInstance`s share the same `MeshRef` (e.g., 100 identical icons), we still issue 100 separate draw calls.

**Current architecture**: The path shader expects a **single** `PathInstance` as a uniform:

```metal
vertex VertexOut path_vertex(
    uint vid [[vertex_id]],
    constant PathVertex *vertices [[buffer(0)]],
    constant PathInstance *instance [[buffer(1)]],  // Single instance uniform
    constant float2 *viewport_size [[buffer(2)]]
)
```

With P0 batching complete, vertices are merged into one buffer, but we still issue N draw calls with N instance buffer uploads:

```
// Current flow for 100 identical icons:
// - 1 vertex buffer upload (merged)
// - 100 instance uploads + 100 drawIndexed calls
```

### Solution

Group by `MeshRef`, use GPU instancing to reduce 100 draw calls â†’ 1 draw call:

```zig
// Conceptual - requires shader changes
fn renderPathsInstanced(self: *Self, paths: []const PathInstance) void {
    // Group by mesh
    var groups: std.AutoArrayHashMap(MeshRef, std.BoundedArray(u32, 256)) = .{};
    for (paths, 0..) |path, i| {
        const list = groups.getOrPutValue(path.getMeshRef(), .{});
        list.append(@intCast(i));
    }

    // Render each group with instancing
    for (groups.iterator()) |entry| {
        const mesh = mesh_pool.getMesh(entry.key);
        const instance_indices = entry.value;

        // Upload instance transforms for this group
        for (instance_indices.slice(), 0..) |path_idx, i| {
            instance_buffer[i] = GpuPathInstance.fromScene(paths[path_idx]);
        }
        uploadInstanceBuffer(instance_buffer[0..instance_indices.len]);

        // Single instanced draw
        drawIndexedInstanced(mesh.index_count, instance_indices.len, 0, 0, 0);
    }
}
```

### Implementation Details

#### 1. Shader Modifications

Change from single-instance uniform to instance buffer with `instance_id` indexing:

```metal
// BEFORE (current):
vertex VertexOut path_vertex(
    uint vid [[vertex_id]],
    constant PathVertex *vertices [[buffer(0)]],
    constant PathInstance *instance [[buffer(1)]],
    ...
) {
    PathInstance inst = *instance;
}

// AFTER (instanced):
vertex VertexOut path_vertex(
    uint vid [[vertex_id]],
    uint iid [[instance_id]],                        // ADD: instance index
    constant PathVertex *vertices [[buffer(0)]],
    constant PathInstance *instances [[buffer(1)]],  // CHANGE: array
    ...
) {
    PathInstance inst = instances[iid];              // Index by instance_id
}
```

This pattern already exists in `text.zig`:

```metal
vertex VertexOut text_vertex(
    uint vid [[vertex_id]],
    uint iid [[instance_id]],
    constant float2 *unit_vertices [[buffer(0)]],
    constant GlyphInstance *glyphs [[buffer(1)]],  // Array indexed by iid
    ...
)
```

#### 2. Instance Grouping (Static Allocation)

To comply with CLAUDE.md's static allocation policy, use fixed-capacity arrays instead of dynamic hash maps:

```zig
const MAX_UNIQUE_MESHES: u32 = 128;
const MAX_INSTANCES_PER_MESH: u32 = 256;

const InstanceGroup = struct {
    mesh_ref: MeshRef,
    instance_count: u32,
    instance_indices: [MAX_INSTANCES_PER_MESH]u16,  // Indices into path array
};

const GroupingState = struct {
    groups: [MAX_UNIQUE_MESHES]InstanceGroup,
    group_count: u32,

    // O(1) lookup via direct indexing into mesh pool indices
    persistent_lookup: [512]u16,  // MAX_PERSISTENT_MESHES -> group index
    frame_lookup: [256]u16,       // MAX_FRAME_MESHES -> group index

    fn findOrCreateGroup(self: *@This(), ref: MeshRef) ?*InstanceGroup {
        const lookup = if (ref.isPersistent())
            &self.persistent_lookup
        else
            &self.frame_lookup;

        const group_idx = lookup[ref.index()];
        if (group_idx != 0xFFFF) {
            return &self.groups[group_idx];
        }
        // Create new group if under limit...
    }
};
```

#### 3. Gradient Handling

The `GradientUniforms` (352 bytes each with 16 color stops) complicates instancing.

**Option A: Storage buffer with indexed lookup**

Pass `instance_id` through vertex output to fragment shader:

```metal
struct VertexOut {
    float4 position [[position]];
    // ... existing fields
    uint instance_index [[flat]];  // Pass to fragment shader
};

fragment float4 path_fragment(
    VertexOut in [[stage_in]],
    constant GradientUniforms *gradients [[buffer(0)]]  // Array
) {
    GradientUniforms grad = gradients[in.instance_index];
    // ...
}
```

**Option B: Limit gradient complexity for instanced paths**

- Solid color paths: Full instancing support
- Simple gradients (â‰¤4 stops): Inline in PathInstance struct
- Complex gradients (>4 stops): Fall back to per-path rendering

#### 4. Draw Call Changes

**Metal:**

```zig
// Current:
encoder.msgSend(void, "drawIndexedPrimitives:indexCount:indexType:indexBuffer:indexBufferOffset:", .{...});

// Instanced - add instanceCount parameter:
encoder.msgSend(void, "drawIndexedPrimitives:indexCount:indexType:indexBuffer:indexBufferOffset:instanceCount:", .{
    @intFromEnum(mtl.MTLPrimitiveType.triangle),
    @as(c_ulong, mesh.index_count),
    @intFromEnum(mtl.MTLIndexType.uint32),
    index_buffer,
    @as(c_ulong, mesh.index_offset * @sizeOf(u32)),
    @as(c_ulong, instance_count),  // NEW
});
```

**WebGPU** (already supports it):

```zig
imports.drawIndexed(index_count, instance_count, first_index, base_vertex, first_instance);
```

### Files to Modify

| File                        | Changes                                                  |
| --------------------------- | -------------------------------------------------------- |
| `path_pipeline.zig`         | Shader source, new `renderPathsInstanced()`              |
| `path_instance.zig`         | Add instance_index field for fragment shader passthrough |
| `scene_renderer.zig`        | Call instanced rendering path                            |
| `web/renderer.zig`          | WebGPU equivalent changes                                |
| `web/shader.wgsl`           | WebGPU shader modifications                              |
| New: `instance_grouper.zig` | Static-allocated grouping logic                          |

### Complexity Estimate

| Component                        | Effort | Risk                              |
| -------------------------------- | ------ | --------------------------------- |
| Shader modifications             | Medium | Low - pattern exists in text.zig  |
| Instance grouping (static alloc) | High   | Medium - careful limit management |
| Gradient storage buffer          | Medium | Medium - fragment shader changes  |
| WebGPU parity                    | Medium | Low - similar pattern             |
| Testing                          | Medium | Low                               |

**Total: ~2-3 weeks of focused work**

### Why Deferred

1. Requires shader modifications (instance_index â†’ instance buffer lookup)
2. Requires sorting/grouping logic with static allocation
3. Gradient handling adds complexity (storage buffer or inline limits)
4. P0 batching gives most of the benefit for typical UI workloads

### When to Implement

- After P0 batching is complete and measured âœ…
- If profiling shows draw call count still dominates
- For specific use cases:
  - **Icon grids**: 100+ identical icons â†’ 1 draw call
  - **Chart markers**: Scatter plots with thousands of identical points
  - **Particle systems**: Many identical particles
  - **Tiled backgrounds**: Repeating pattern elements

### Recommendation

1. **Measure first**: Add GPU performance counters to determine if draw calls are the actual bottleneck
2. **Start simple**: Implement for solid-color paths only (skip gradient complexity initially)
3. **Consider hybrid**: Keep per-path rendering for complex gradients, use instancing for solid/simple cases

---

## P3: Adaptive Circle LOD âœ… COMPLETE

### Problem

All circles use the same segment count regardless of screen size:

```zig
// Current: fixed segments via ellipse()
pub fn circle(self: *Self, cx: f32, cy: f32, r: f32) *Self {
    return self.ellipse(cx, cy, r, r);
}
```

### Solution

```zig
pub fn circleAdaptive(self: *Self, cx: f32, cy: f32, r: f32, pixels_per_unit: f32) *Self {
    const screen_radius = r * pixels_per_unit;

    // Segment count based on screen coverage
    const segments: u8 = if (screen_radius < 2) 4      // Tiny: square is fine
        else if (screen_radius < 8) 6                   // Small: hexagon
        else if (screen_radius < 32) 12                 // Medium
        else if (screen_radius < 128) 24                // Large
        else 32;                                        // Very large

    return self.circleWithSegments(cx, cy, r, segments);
}

pub fn circleWithSegments(self: *Self, cx: f32, cy: f32, r: f32, segments: u8) *Self {
    std.debug.assert(segments >= 3);

    const delta = std.math.tau / @as(f32, @floatFromInt(segments));

    _ = self.moveTo(cx + r, cy);
    for (1..segments) |i| {
        const angle = delta * @as(f32, @floatFromInt(i));
        _ = self.lineTo(cx + r * @cos(angle), cy + r * @sin(angle));
    }
    return self.close();
}
```

### Implementation Tasks

#### Task P3.1: Add `circleWithSegments` âœ…

**File:** `gooey/src/core/path.zig`

Added `circleWithSegments` and `ellipseWithSegments` for explicit segment control.

#### Task P3.2: Add `circleAdaptive` âœ…

**File:** `gooey/src/core/path.zig`

Added `circleAdaptive` and `ellipseAdaptive` with automatic LOD based on screen radius.

#### Task P3.3: Update Canvas API âœ…

**File:** `gooey/src/ui/canvas.zig`

Added `fillCircleAdaptive`, `fillEllipseAdaptive`, and `fillCircleWithSegments`. Uses `DrawContext.scale` for LOD calculation.

### Acceptance Criteria

- [x] Small circles (< 8px radius) use fewer segments
- [x] Large circles remain smooth
- [x] No visual artifacts at LOD boundaries

---

## Implementation Schedule

### Week 1: P0 â€” WebGPU Batching

- Day 1-2: Add staging buffers, implement `renderPathsBatched`
- Day 3: Handle chunking for overflow
- Day 4: Batch gradient uniforms
- Day 5: Testing and measurement

### Week 2: P1 â€” Reflex Tracking âœ… COMPLETE

- ~~Day 1: Add bitset, pre-compute reflex vertices~~
- ~~Day 2: Modify `hasPointInside` to use bitset~~
- ~~Day 3: Implement neighbor update on ear removal~~
- ~~Day 4-5: Testing, edge cases~~

### Week 3: P3 â€” Polish âœ… COMPLETE

- ~~Day 1-2: Adaptive circle LOD~~
- ~~Day 3-5: Profiling, documentation, any P2 prep~~

---

## Measurement Plan

### Baseline Metrics (capture before starting)

```zig
// Add to debug builds
const PathRenderStats = struct {
    buffer_uploads: u32 = 0,
    draw_calls: u32 = 0,
    total_vertices: u32 = 0,
    total_indices: u32 = 0,
    triangulation_time_us: u64 = 0,
};
```

### Test Scenarios

1. **Simple UI** (10 paths): Verify no regression
2. **Complex Canvas** (100 paths): Primary optimization target
3. **Stress Test** (1000 paths): Chunking validation
4. **Repeated Icons** (100 identical): P2 baseline

### Success Criteria

| Metric                       | Before | After P0 | After P1 |
| ---------------------------- | ------ | -------- | -------- |
| API calls (100 paths)        | ~400   | ~103     | ~103     |
| Triangulation (500 vertices) | ~125ms | ~125ms   | ~5-50ms  |
| Frame time (100 paths)       | TBD    | -30%+    | â€”        |

---

## Files to Modify

```
gooey/src/platform/wgpu/web/renderer.zig   # P0: Batch rendering
gooey/src/core/triangulator.zig            # P1: Reflex tracking
gooey/src/core/path.zig                    # P3: Adaptive LOD
gooey/src/ui/canvas.zig                    # P3: Pass scale to path
```

## Non-Goals (Removed from Original Assessment)

These items were analyzed and determined to not be worth implementing:

| Item                     | Reason                                              |
| ------------------------ | --------------------------------------------------- |
| Precomputed shape meshes | `fillRect` already uses optimized Quad primitives   |
| Gradient binary search   | 4 vs 8 comparisons on GPU is negligible             |
| Stroke triangle strips   | Complexity outweighs 33% index reduction            |
| SIMD Vec2                | Scattered 2D ops don't benefit; compiler handles it |

---

## Notes

- All new buffers must follow static allocation policy (CLAUDE.md)
- Staging buffers should use `[N]T = undefined` pattern, not ArrayList
- Keep assertions: vertex counts, index bounds, buffer overflow
- Test on both Metal and WebGPU after changes
